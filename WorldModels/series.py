import numpy as np
import os
import json
import tensorflow as tf
import random
# from vae.vae import CVAE
from utils import PARSER

gpu_devices = tf.config.experimental.list_physical_devices('GPU')
for gpu in gpu_devices:
    tf.config.experimental.set_memory_growth(gpu, True)

args = PARSER.parse_args()
DATA_DIR = "results/{}/{}/records".format(args.exp_name, args.env_name)
SERIES_DIR = "results/{}/{}/seriess".format(args.exp_name, args.env_name)    ## This is the data that is used to train the RNN, in the original paper, it containes data processed by the VAE
# model_path_name = "results/{}/{}/tf_vae".format(args.exp_name, args.env_name)
num_eps = 10000#150#   #TODO: what is this for? Should it come from config? - would rather not. Was experimenting on it.

num_agents = args.num_agents
indices = tf.range(num_agents)
one_hot_encoding = np.array(tf.one_hot(indices, num_agents))

if not os.path.exists(SERIES_DIR):
    os.makedirs(SERIES_DIR)

def ds_gen():
    filenames = os.listdir(DATA_DIR)[:num_eps] # only use first 10k episodes
    n = len(filenames)
    i = 0
    for j, fname in enumerate(filenames):
        if not fname.endswith('npz'): 
            continue
        
        file_path = os.path.join(DATA_DIR, fname)

        # TODO: extract observations agent by agent, use keyword "agent" and walker_0, walker_1, etc...
        data = np.load(file_path)
        agents = data['agent']
        agents = np.transpose(np.tile(one_hot_encoding,int(len(agents)/num_agents)))

        z = data['obs']
        z = np.concatenate((agents, z), axis = 1)
        action = np.reshape(data['action'], newshape=[-1, args.a_width])
        reward = data['reward']
        done = data['done']
        if done.shape[0] > (num_agents*args.max_frames + num_agents):
            i+=1
            continue
        #   print(fname, z.shape,action.shape, reward.shape, done.shape)
        #   print(z[done.shape[0]-10:done.shape[0]+1,0],done[done.shape[0]-10:done.shape[0]+1])
        n_pad = num_agents*(args.max_frames+1) - z.shape[0] # pad so they are all a thousand step long episodes
        z = tf.pad(z, [[0, n_pad], [0, 0]])
        action = tf.pad(action, [[0, n_pad], [0, 0]])
        reward = tf.pad(reward, [[0, n_pad]])
        done = tf.pad(done, [[0, n_pad]], constant_values=done[-1])
        yield z, action, reward, done
    print("Skipped: ", i)

## Creates a dataset whose elements are generated by ds_gen
# TODO: update to ouput 3 datasets (or create 3 separate dataset functions)
def create_tf_dataset():
    dataset = tf.data.Dataset.from_generator(ds_gen, output_types=(tf.float32, tf.float32, tf.float32, tf.bool), output_shapes=((num_agents*(args.max_frames+1), args.z_size+num_agents), (num_agents*(args.max_frames+1), args.a_width), (num_agents*(args.max_frames+1),), (num_agents*(args.max_frames+1),)))
    return dataset

'''
## with VAE
# @tf.function
# def encode_batch(batch_img):
#   simple_obs = batch_img/255.0
#   mu, logvar = vae.encode_mu_logvar(simple_obs)
#   return mu, logvar

# def decode_batch(batch_z):
#   # decode the latent vector
#   batch_img = vae.decode(z.reshape(batch_size, z_size)) * 255.
#   batch_img = np.round(batch_img).astype(np.uint8)
#   batch_img = batch_img.reshape(batch_size, 64, 64, 3)
#   return batch_img
'''


@tf.function
def encode_batch(batch_obs):
  mu = tf.math.reduce_mean(batch_obs, axis=0) 
  logvar = tf.math.log(tf.math.reduce_variance(batch_obs, axis=0))
  return mu, logvar

filelist = os.listdir(DATA_DIR)
filelist.sort()
filelist = filelist[:num_eps]
dataset = create_tf_dataset()
dataset = dataset.batch(1, drop_remainder=True)
# vae = CVAE(args=args)
# vae.set_weights(tf.keras.models.load_model(model_path_name, compile=False).get_weights())

mu_dataset = []
logvar_dataset = []
action_dataset = []
r_dataset = []
d_dataset = []
# N_dataset = []

i=0
for batch in dataset:
  i += 1
  obs_batch, action_batch, r, d = batch
  obs_batch = tf.squeeze(obs_batch, axis=0)
  action_batch = tf.squeeze(action_batch, axis=0)
  r = tf.reshape(r, [-1, 1])
  d = tf.reshape(d, [-1, 1])
  mu, logvar = [],[]
  for j in range(num_agents):
    mus, logvars = encode_batch(obs_batch[j::num_agents,:])
    mu.append(mus)
    logvar.append(logvars)
  mu, logvar = tf.stack(mu), tf.stack(logvar)
#   print(mu[:,:3])
  mu_dataset.append(mu.numpy().astype(np.float32))
  logvar_dataset.append(logvar.numpy().astype(np.float32))
  action_dataset.append(action_batch.numpy())
  r_dataset.append(r.numpy().astype(np.float32))
  d_dataset.append(d.numpy().astype(np.bool_))

  if ((i+1) % 100 == 0):
    print(i+1)

mu_dataset = np.array(mu_dataset)
logvar_dataset = np.array(logvar_dataset)
action_dataset = np.array(action_dataset)
r_dataset = np.array(r_dataset)
d_dataset = np.array(d_dataset)
print("Shapes: ", mu_dataset.shape, logvar_dataset.shape, r_dataset.shape)

np.savez_compressed(os.path.join(SERIES_DIR, "series.npz"), action=action_dataset, mu=mu_dataset, logvar=logvar_dataset, reward=r_dataset, done=d_dataset)
